\chapter{Preparation}

This chapter describes the necessary background material required to understand Paxos; first in the simpler case of agreeing on one value and then going on to the Multi-decree case that will be implemented. From this a set of requirements for the project is established, taking into account functionality that is glossed over in theoretical descriptions of the algorithm. Finally the software engineering aspects of the project will be discussed including the choice of tools, build processes and third party libraries.

\section{Theotrical background}

\subsection{Assumptions of the environment}
When considering developing a system with distributed consensus, it is necessary to consider the assumptions made in the environment in which such a system will operate. This is to ensure there is enough functionality embedded in the conesnsus system to ensure that consensus is reached under a given set of assumptions. \\

A \emph{process} (or networked process) is an instance of the program running on a networked machine in a distributed system. Assumptions of these processes that participate in the system:

\begin{itemize}
  \item Processes can undergo \emph{crash failures}. A crash failure is defined as a process terminating but not entering an invalid state.
  \item Processes may recover from crash failures. They can rejoin the system in some valid state. 
  \item Processes operate at arbitrary speeds. This cannot be distinguished by other processes from arbitrarily long delays in the network.
\end{itemize}

Assumptions of the network in which these processes communicate:

\begin{itemize}
  \item All processes can communicate with one another.
  \item The network environment is \emph{asynchronous}. That is, messages may take an arbitrarily long time to be delivered.
  \item Messages may be re-ordered upon delivery.
  \item Messages may be duplicated in the network.
  \item Messages may be dropped from the network.
  \item Messages are not corrupted or modified in the network.
  \item A message that is received by one process was, at some point in the past, sent by another process.
\end{itemize}

% OLD LABELLED BULLETS
%\storestyleof{itemize}
%\begin{listliketab}
%\noindent
%\begin{tabularx}{\linewidth}{@{}LXl@{}}
%    \textbullet & All processes are connected on the network. & (N1) \\
%     \textbullet & The network environment is \emph{asynchronous}. That is, packets may take an arbitrarily long time to be delivered. & (N2) \\
%       \textbullet & Packets may be re-ordered upon delivery, duplicated or dropped from the network. & (N3) \\
%  \textbullet & Packets are not corrupted or modified in the network. & (N4) \\
%  \textbullet & A packet that is received by one process was, at some point in the past, sent by another process. & (N5)
%  \end{tabularx}
%\end{listliketab}

The last two assumptions assume a system that does not tolerate what are known generally as \emph{Byzantine failures}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Aim of consensus}

With distributed consensus, we wish for a network of processes to agree on some value. In consensus algorithms it is assumed that processes can somehow propose values to one participating processes. The goal of distributed consensus is, given a number of processes that can each propose some value $v$, that one of the proposed values is chosen. This is the \emph{single-decree} case, that is only one value is proposed by each process and only one is chosen. \\

In the \emph{multi-decree} case, agreement is reached over a sequence of values. That is, each process will propose a sequence of valus $v_1, v_2, \ldots, v_n$ and the role of the consensus protocol is to have the system choose one such sequence from all those proposed. This multi--decree case allows for the state machine replication technique to be employed to replicate an application across a number of machines in a distributed system.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{State machine replication}


A desirable goal of distributed computing is to replicate an application across a number of machines so that each \emph{replica} has the same strongly--consistent view of the application's state. This technique is referred to as State machine replication (SMR); it leads to both for increased fault tolerance and higher availability. Multi--decree consensus protocols provide a primitive by which an application (that behaves like a state machine) can be replicated. \\

Each process participating in the consensus protocol runs the replicated state machine application, with each process starting in the same state. Then by treating the values proposed in the consensus protocol as \emph{commands} to perform a state transition, then by running a consensus protocol each process will receive the same serialized sequence of commands $c_1, c_2, \ldots, c_n$. These commands are treated as commands to perform a state transition and as such each process perform the same sequence of transitions from the same starting state and thus be a replica of the the state machine application. \\

Before considering how to implement SMR in the multi--decree case, it is useful to examine how Paxos operates in the simpler single--decree case. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Single--decree Paxos}

\begin{table}
  \centering
  \begin{tabular}{l|p{8cm}|c}
    Role & Purpose & Number required \\ \hline \hline
    Proposer & Propose values to acceptors. & $f+1$ \\
                     & Send prepare requests with proposal numbers. & \\ [.5\normalbaselineskip] \hline
    Acceptor & Decide whether to \emph{adopt} a proposal based on its proposal number & $2f+1$ \\
                     & Decide whether to \emph{accept} a proposal based on a higher numbered proposal having arriving. & \\ [.5\normalbaselineskip] \hline
    Learner & Learn value chosen by majority of acceptors & $f + 1$ \\ [.5\normalbaselineskip]
  \end{tabular}
  \caption{Summary of the roles in single--decree Paxos. In this description the system can tolerate the failure of up to $f$ of each given role.}
\label{table:role-summary}
\end{table}

Single--decree Paxos is the variant of the algorithm that allows for a single value to be chosen from a set of proposals and provides a foundation for the multi--decree case that will be considered next. The terminology used here follows Lamport's paper \cite{paxos-made-simple} describing the single--decree protocol in simple terms. Processes take the roles of \emph{proposers}, \emph{acceptors} and \emph{learners}, each of which has a designated task in the algorithm. In reality these roles are often co-located within a single process but it is simpler to consider each separately.  The prupose of each role and the number of each role required to tolerate $f$ failures is summarised in Table \ref{table:role-summary}.  \\

Proposers that wish to propose a value $v$ submit proposals of the form $\left(n,v\right)$, where $n \in \mathbb{N}$ is called a proposal number. Each proposer may propose one proposal at a time and may only use strictly increasing proposal numbers for each proposal. Furthermore, each proposer must use a disjoint set of proposal numbers. The Paxos algorithm is divided into a number of stages described below. \\

\textbf{Phase 1a (Prepare phase)} A proposer wishing to propose a value first sends a \texttt{prepare($n$)} message to a majority of the set of acceptors, where $n$ is the highest proposal number it has used so far. \\

\textbf{Phase 1b (Promise phase)} In this phase an acceptor receiving a \texttt{prepare($n$)} message must decide whether or not to \emph{adopt} this proposal number. Adopting a proposal number is the act of promising not to accept a future proposal number $n'$ such that $n' < n$. The acceptor will adopt $n$ if it is the highest proposal number it has received thus far, in which case it will reply to the proposer with a \texttt{promise($n''$,$v$)} message, where $n''$ is the highest proposal number it has previously accepted and $v$ is the corresponding proposal's value. Otherwise, it can simply ignore the proposer or send a \texttt{NACK} message so the proposer can abandon the proposal. \\

\textbf{Phase 2a (Accept phase)} Upon receipt of a \texttt{promise($n$,$v$)} message from a majority of the set of acceptors, the proposer replies to each with an \texttt{accept($n'$,$v'$)}, where $n'$ is the highest proposal number returned by the acceptors in the promise phase and $v'$ is its corresponding value. \\

\textbf{Phase 2b (Commit phase)} An acceptor receiving a \texttt{accept($n$,$v$)} message from a proposer will decide whether to commit the proposal for $v$. If the acceptor hasn't made a promise to adopt a proposal number higher than $n$, then it will commit $v$, otherwise it will ignore this message or send a \texttt{NACK} to the proposer. \\

Figures \ref{fig:single-paxos-1} and \ref{fig:single-paxos-2} show two example message flows between proposers and acceptors in the single Paxos protocol. \\

Once this process is completed, a majority of the acceptors will have chosen the same proposed value. Learners are required to each learn the value chosen by the majority. A number of different methods can be employed achieve this. The simplest is to have acceptors, on choosing a value to accept, broadcast their decision to the set of learners. \\

\begin{figure}
  \centering
  \input{include/single-paxos.latex}
  \caption{Timing diagram showing a single proposer $p_1$ committing a value $v$ with three acceptors $a_1$, $a_2$ and $a_3$}
  \label{fig:single-paxos-1}  
\end{figure}

\begin{figure}
  \centering
  \input{include/single-paxos-2.latex}
  \caption{Timing diagram showing a proposer $p_1$ being preempted by acceptors $a_1$, $a_2$ and $a_3$ having already adopted a higher proposal number submitted previously by $p_2$}
  \label{fig:single-paxos-2}  
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Multi--decree Paxos}
\label{subsection:multi-decree-paxos}

Single--decree Paxos can be naively extended by allowing proposers to propose values one at a time. However, this is wasteful as it requires that proposers send \texttt{prepare} messages for each proposal they wish to make. Van Renesse et al \cite{VanRenesse:2015:PMM:2737799.2673577} describes Multi-Paxos with an emphasis on use for state machine replication that forms the basis of the description that follows. In this paper the roles of the processes are changed to emphasise the purpose of the system SMR. \emph{Replica} processes are introduced to implicitly act as the learners and maintain the replicated state machine. \emph{Client} processes are introduced as a means to externally submit commands to the replicas. Proposers are referred to as \emph{leaders} as each takes the lead over some set of \emph{ballots}. The new roles and their correspondence to the single--decree roles are summarised in Table \ref{table:multi-role-summary}. \\

%\begin{figure}
%  \centering
%  \input{include/comm-patterns.latex}
%  \caption{...}
%  \label{fig:multi-paxos-comm-patterns}  
%\end{figure}

\begin{table}
\centering
\begin{tabular}{ | l  | l | p{6cm} | c | } \hline
   \textbf{Role} & \textbf{Equivalent} & \textbf{Purpose} & \textbf{Number required} \\ \hline
   Client & N/A & Send commands to replicas and receive responses & $ N/A $ \\ \hline
  
   Replica & Learner & Receive requests from clients. &  \\
                 & & Serialize proposals and send to leaders. & $f + 1$ \\
                 & & Receive decisions and apply to the replicated application state. & \\
                 & & Handle reconfiguration of set of leaders & \\ \hline
  
  Leader & Proposer & Request acceptors adopt ballots. & $f + 1$ \\
               & & Secure adoption of ballots in phase 1 of synod protocol & \\
               & & Secure acceptance of ballots in phase 2 &  \\ \hline
               
  Acceptor & Acceptor & Adopt the highest ballot they have received & $2f + 1$ \\
                   & & Accept pvalues of ballot they have adopted & \\
                   & & Fault tolerant distributed memory of pvalues &  \\ \hline
\end{tabular}
\caption{Summary of the roles in Multi Paxos. In this description the system can tolerate the failure of up to $f$ of each given role. Clients do not explicitly participate in the protocol and so there is no requirement on any number being live at any given time.}
\label{table:multi-role-summary}
\end{table}

\subsubsection{Clients}

The purpose of clients is to allow for commands to be sent externally to the system which can then be formed into proposals internally. This allows the system to behave in a manner more like that of a typically deployed distributed system (with a client / server architecture) and provides a degree of failure transparency. \\

A command $c$ takes the form $\left( \kappa, \mathrm{\emph{cid}}, \mathrm{\emph{op}} \right)$, where $\kappa$ is a unique identifier for the client, \emph{cid} is a unique identifier for the client's sent commands and \emph{op} is the operation the command should perform. Clients broadcast a \texttt{request($c$)} message to the replicas and each is issued a \texttt{response(}\emph{cid}\texttt{,}\emph{result}\texttt{)} when consensus is reached and it has been applied to each replica's application state.

\subsubsection{Replicas}

Replicas receive commands and attempt to serialize them by converting each command $c$ into a proposal $\left(s,c\right)$, where $s \in \mathbb{N}$ is a slot number. The slot number describes ordering of the sequence in which the commands should be committed; this is not to be confused with the proposal number $n$ in the single-decree protocol. \\

Different replicas may form different sequences of proposals and so broadcasts a \texttt{propose($s$,$c$)} message to the set of leaders and awaits a \texttt{decision($s'$,$c'$)} message. The resulting decision may differ in its slot number and so the replica may have to re-propose a command it has proposed for the decided slot. Upon receipt of decisions the replica will apply the associated operation to the application state, maintaining the replicated application.

\subsubsection{Ballots and pvalues}

Ballots are the structure over which leaders and acceptors operate when participating in the synod protocol. Ballots provide indirection in that they allow for leaders to secure agreement with acceptors for a number of proposals with different slot numbers or for a given slot to be targeted by a number of proposals associated with different ballots. This allows for a leader to perform the first phase of the synod protocol independently of having received any new proposals. It also increases the ability to execute the protocol concurrently for a number of slots at once. \\

Each ballot is uniquely identified by a \emph{ballot number}. These are either pairs $\left(r, \lambda\right)$ (where $r \in \mathbb{N}$ is called a round number and $\lambda$ is a leader's unique identifier) or $\bot$, a specially designated least ballot number. \\

\emph{Pvalues} associate a ballot number with proposal. They are triples $\left(b, s, c \right)$ consisting of a ballot number, a slot number and a command. These are analogous to to the $\left( n, v \right)$ pairs used in the single-decree case. We previously required that each proposer used a disjoint subset of $\mathbb{N}$ for their proposal numbers. We can avoid this requirement as each ballot number encodes the identifier of the leader directly in its ballot number. Hence no two leaders can generate equal ballot numbers.\\

We require ballot numbers to be totally ordered so that acceptors can compare which ballot number is less than another when choosing whether to adopt or accept. Letting $\mathcal{B}$ denote the set of all ballot numbers, we define the relation $\leq_{\mathcal{B}} \ \in \mathcal{B} \times \mathcal{B}$ which satisfies the following two conditions:

\begin{gather*}
  \forall \left( n, \lambda \right), \left( n', \lambda' \right) \in \mathcal{B} .
 \ \left( n, \lambda \right) \leq_{\mathcal{B}} \left( n', \lambda' \right) \iff
   \left ( n \leq n' \right) \vee \left( n = n' \wedge \lambda \leq_{\Lambda} \lambda' \right) \\
\forall \ b \in \mathcal{B} \ . \ \bot \leq_{\mathcal{B}} b
\end{gather*}  

Note this implies that we require leader identifiers be equipped with a total order relation  $\leq_{\Lambda}$ as well. \\

\subsubsection{Quorums}

%Unlike replicas, acceptors do not each attempt to store the same sequence of proposals. We requirement that progress is still made given a minority of acceptors fail; that is $f + 1$ failures for a given set of $2f + 1$ acceptors. \\

%We hence require that for each leader ...

%Explanation of quorum systems. \\

Unlike replicas, acceptors do not each attempt to store their own consistent copies of proposals. Instead, it is required that at least a majority of the acceptors have in their memory a decision for a committed proposal. This is so that if any minority of the acceptors crash then there is still going to be at least one acceptor with memory of the committed proposal. A subset of the acceptors  $Q \subseteq \mathcal{A}$, where $|\mathcal{A}| = 2f + 1$ for some $f \in \mathbb{N}$, is called a \emph{majority quorum} or just \emph{quorum} if $|Q| = f + 1$. \\

Hence at each phase of the synod protocol leaders broadcast their message to all acceptors in a configuration and wait for such a quorum of responses. This is to guarantee that if any minority of the acceptors fail in the future that at least one will have retained the result of the current round of the protocol.

\subsubsection{The Synod Protocol}

The synod protocol is the protocol undertaken by the set of leaders and acceptors in order to decide which command is committed to which slot. The protocol proceeds in two phases similarly to the single-decree case except now leaders and acceptors operate over pvalues. \\

Acceptors maintain a ballot number in their state their represents the last ballot number they have \emph{adopted}. Adopting a ballot number represents a promise not to accept any pvalues that do not have that ballot number, so that acceptors do not accept conflicting pvalues. Acceptors \emph{accept} a pvalue by adding it to the set of pvalues, initially empty, that they have already accepted. The algorithm requires that if two acceptors $\alpha, \alpha' \in \mathcal{A}$ have accepted $(b,s,c)$ and $(b,s,c')$ then $c = c'$. \\

In the absence of receiving any \texttt{propose($s$,$c$)} messages from replicas, leaders attempt to secure an initial ballot with ballot number $\left(0, \lambda_i)$, where $\lambda_k$ is the identifier of the $k^{\text{\tiny th}}$ leader. They do this by broadcasting a \texttt{phase1a($b$)} message in the same format as that below. \\

\textbf{Phase 1a} Leaders attempt to secure adoption of a ballot $b$ by broadcasting a \texttt{phase1a($b$)} message to the set of acceptors. \\

\textbf{Phase 1b} Acceptors receiving a \texttt{phase1a($b$)} compare $b$ to the highest ballot number $b'$ they have adopted thus far. This is initially $\bot$ so acceptors will always adopt the first ballot number they receive. If $b' \leq_\mathcal{B} b$, then the acceptor will adopt this new ballot number, that is set $b' := b$. In either case, the acceptor will reply with a \texttt{phase1b($b'$, pvals)} message, where $\textrm{pvals}$ is the set of pvalues previously accepted. \\

If a leader receives \texttt{phase1b($b'$, pvals)} from a majority quorum of acceptors and if $b = b'$ then the leader will proceed with phase 2 of the protocol. Otherwise the leader abandons the proposal, increments the round number of its ballot number and tries again. If a leader receives a response where $b' \neq b$ then it should restart the protocol with a higher round number. \\

\textbf{Phase 2a} Leaders proceed to commit a given pvalue $\left( b, s, c \right)$ by broadcasting a \texttt{phase2a($b,s,c$)} message to the set of acceptors. \\

\textbf{Phase 2b} Acceptors receiving a \texttt{phase2a($b,s,c$)} will compare $b$ to their adopted ballot number $b'$. If $b = b'$ then they will accept the pvalue $(b,s,c)$; otherwise they do not change their state. In either case they reply with a  \texttt{phase2b($b'$)} message. \\

Leaders will again wait for receipt of a \texttt{phase2b($b'$)} from a majority quorum of acceptors. If $b = b'$ then they know the acceptor has accepted the pvalue and so broadcast a \texttt{decision($s,c$)} message to the set of replicas so they can commit command $c$ to slot $s$. Otherwise if $b \neq b'$ then the acceptor knows the acceptor has since adopted a different leader's ballot number and so should restart the protocol with a higher round number. \\
 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Requirements}

\subsection{System requirements}

As noted previously, there are a number of papers that describe the Paxos protocol and its variants. Many of these sources present the algorithm in a different format and nearly all of them present it in a theoretical setting, with little concern for functionality that is generally required in software. Implementation details such as how each of the participating process knows how to address one another are often entirely overlooked. Hence, in developing an implementation of Multi-Paxos, it is necessary to formally recognise the theoretical requirements from the literature as well as identify the additional functionality required to realise the algorithm as a functioning piece of software. \\

It is therefore necessary to perform a \emph{requirements analysis} of the final system. This was performed by collecting information from the literature on how the algorithm is presented and also considering all of the necessary functionality required to implement such an algorithm. For example, in the Paxos Made Moderately Complex paper processes are described as being able to send messages between one another. This of course presents the requirement for a messaging primitive that operates on top of a best--effort IP network. The final system requirements are listed below. \\

\textbf{General requirements}
\begin{itemize}
  \item The ability to pass the executable file some command line arguments for initializing the process.
  \item Ability for each node to be nominated to a given role and to start-up in that role (via the command line).
  \item Configuration files that contain the network addresses of each of the processes participating. Each process must be able to parse such a configuration file.
  \item The ability to log important information to disk or direct to \texttt{stdout} / \texttt{stderr}. The ability to log different information (e.g. debugging info versus error info) to different files.
\end{itemize}

\textbf{Messaging subsystem}
\begin{itemize}
  \item Provide an abstraction over the network so processes can send messages, rather than worrying about functionality of network.
  \item Send messages as required by the Multi-Paxos algorithm, being able to serialise and deserialise arguments and results.
  \item Mask failures in the network from processes
\end{itemize}

\textbf{Application requirements}
\begin{itemize}
  \item A key value store application that can be modelled as a state machine. This needs to have commands to create key value pairs, update key values pairs, read a value for a given key and delete a key value pair for a given key.
  \item The store must be strongly consistent across all replicas.
\end{itemize}

\textbf{Consensus algorithm requirements}
\begin{itemize}
  \item Clients processes can messages with commands to the replicas and receive responses.
  \item Replicas that each maintain a strongly consistent copy of the application state. They must receive requests from clients and each attempt to propose their own serialisation of the sequence of the requests to leaders.
  \item Leaders and acceptors take part in the Synod protocol that produces a consistent serialisation of the commands proposed by replicas.
\end{itemize}

%\subsection{Testing and evaluation requirements}
%
%It is crucial that there is a system to check that the algorithm behaves as expected in the environement described in the assumptions. \\
%
%Talk about using Mininet and the built in scripting ability to provide such a test harness. \\
%
%\textbf{Test harness}
%\begin{itemize}
%  \item ...
%  \item ...
%  \item ...
%\end{itemize}

%{\color{green} \subsection{Extension elements}
%Talk about FPaxos here (when I've actually built it.)}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Software engineering}

This section gives detail on the software development pratices and related work that was undertaken to let development commence. The development methods, third party libraries, tools and test strategies that were employed to manage the project and improve productivity are discussed here.

\subsection{Starting point}

The starting point of this project is the same as that described in the Project Proposal reproduced in Appendix \ref{appendix:proposal}.

\subsection{Methodology}

The method by which the software was developed was to split the system into its logical constituents and develop each in turn and with as much independence from the rest as possible. To facilitate this and from the requirements I deemed it necessary to separate the messaging system from the actual operation of Multi--Paxos itself. \\

This messaging system was the first module to be developed and was incrementally equipped with functionality that facilitated the rest of the program. The next logical division was to separate the operation of clients and replicas from that of leaders and acceptors. Clients, replicas and associated functionality was developed first followed by that of leaders and acceptors in two phases of development. \\

\subsection{Libraries}

OCaml has a large eco-system of third party libraries available. They are used where it is necessary to avoid writing large bodies of code for common tasks. This project uses the OPAM\footnote{https://opam.ocaml.org} package manager to install, update and manage any third party libraries via a command line interface. \\ 

A library of particular interest to this project is that which provides RPC (remote procedure call) functionality. Cap'n Proto\footnote{https://capnproto.org/} was chosen as it has good support and documentation for its OCaml bindings; separated into Capnp-ocaml\footnote{https://github.com/capnproto/capnp-ocaml} for serialization and Capnp-rpc\footnote{https://github.com/mirage/capnp-rpc} for RPCs itself. The library provides a schema language to describe the required remote procedures and any required structures and a code generator for generating OCaml bindings. \\

Since this project is in the space of distributed systems it is necessary to consider how concurrent programming would be achieved. Lwt\footnote{https://github.com/ocsigen/lwt} is a monadic concurrency library that leverages the type system to express the promise of a computation, of type \texttt{'a Lwt.t}, that returns a value of type \texttt{'a}. Monadic functions \texttt{return} and \texttt{bind} to chain computations together in a safe manner and support for parallel computation, mutexes and Unix bindings. This library was chosen because it has thorough documentation and interoperates with Cap'n Proto. \\

Whilst OCaml already comes with a standard library, this project uses Core\footnote{https://github.com/janestreet/core} as a popular overlay. Core improves upon the standard library by providing a number of additional useful functions and modules and also in some places improves the type definitions of those present in the default standard library (by introducing named arguments in order to improve readability). Yojson\footnote{https://github.com/mjambon/yojson} was used for serialization, both in configuration files and in some areas sending data in messages. OUnit\footnote{http://ounit.forge.ocamlcore.org/} was used to manage unit testing for this project and is discussed in \ref{subsection-testing}. \\

In the case of each library used their software lisence was made readily available either by OPAM (via the \texttt{opam show <pkg-name>} command) or by licensing information made available on a library's Github repository. Each of the libraries used had licenses that made use of the software for this project possible, with YoJson and Uri requiring reproduction of their licensing information distributed with the project. Table \ref{table:software-licenses} gives details of each library's individual license.

\begin{table}
\centering
\begin{tabular}{ | l  | p{3.0cm}| p{7.0cm} | } \hline
   \textbf{Library} & \textbf{License} & \textbf{Information availabble} \\ \hline
    Lwt & LGPL with OpenSSL linking exception & \texttt{opam show lwt} \\ \hline
    Core & Apache-2.0 & \texttt{opam show core} \\ \hline
    YoJson & Specific license file & \url{https://github.com/mjambon/yojson/blob/master/LICENSE.md} \\ \hline
    OUnit & Specific license file & \url{https://github.com/mjambon/yojson/blob/master/LICENSE.md} \\ \hline
    Ocaml-capnp & Apache &  \texttt{opam show ocaml-capnp} \\
    Capnp-rpc & & \texttt{opam show capnp-rpc} \\ \hline
\end{tabular}
\caption{Software licenses for each of the libraries used in the project.}
\label{table:software-licenses}
\end{table}

\subsection{Build system}
JBuilder was used as the build system for this project. It was chosen for its minimal configuration files (called jbuild files), automatic linking of OPAM packages and automates the generation of \texttt{.merlin} files. Jbuild files are written in a simple S-expression syntax and are composable with one another. Jbuild files also present the ability to include \emph{rules} so that Cap'n Proto OCaml bindings are generated automatically when building the project.

\subsection{Tools}
  
Choice of tools is important for being productive in a language. MacVim\footnote{https://github.com/macvim-dev/macvim} was used as a text editor. In order to improve productivity Merlin was used to provide code-completion and edit-time type inference features, making it much easier to understand the types of source code and save time reading documentation online. \texttt{.merlin} files are auto-generated by Jbuilder that allow for code completion and type inference over files in the project directory itself. \\

Git was used for source control and version. Branches were made for each new feature under development and merged back into the master. Backups were to a remote repository on Github. Backups were also made periodically to Google Drive. All of the development work was undertaken on my person machine with the option of fallback onto the MCS machines. \\

In order to simulate the operation and performance of instances of the implementation in a distributed system it was necessary to use a network simulator. Mininet\footnote{http://mininet.org/} is a network simulator that operates with proess based virtualisation and allows for arbitrary processes to be simulated as running on hosts on a virtual network. Mininet provides a complete Python API for setting up such simulations and performing experiments. Mininet required use of a guest operating system on VirtualBox\footnote{https://www.virtualbox.org/}, virtualization software that allowed Mininet to be used on my own machine.

\subsection{Testing}
\label{subsection-testing}
Unit testing was performed in order to check the operation of functions at a fine-grained level within the program. OCaml's static type system ensures the compiler will catch a number of errors resulting from improper calls to functions and so unit tests were designed as a compliment to this; tests focussed on correctly typed arguments to functions yielding appropriate results we expect. The test suites were executed after successful builds of the project.

% {\color{red}Simulation testing.}

\section{Summary}
This chapter discussed the theoretical basis for implementing Multi--Paxos and state machine replication before going on to describe a set of requirements for an implementation of the algorithm, finishing with a description of the software engineering tools and techniques employed. With this in place it is possible to go on to describe the implementation of the algorithm and its underlying systems in Chapter 3.





