\chapter{Implementation}

This chapter describes the implementation. It begins with a high level overview of the structure of the program and initialisation. This is followed by an examination of the data structures that are used and goes on to describe the messaging system. Finally, each of the roles in the system is described in turn.

\section{High level architecture}

\begin{figure}
\centering
\scalebox{.7}{\input{include/systems.latex}}
       \caption{High level subsystems of a process}
       \label{fig:high-level-structure}
  \centering
\end{figure}

This section describes the architecture of the program. Each process participating in Multi--Paxos is an instance of the same executable configured to take the \emph{role} of either client, replica, leader or acceptor. These roles each have a different function in the system within their \emph{role module}, which encapsulates all the functionality associated specifically with a given role. Each of these modules will need to send and receive messages of some form; this is abstracted over by a \emph{messaging system} which exposes an interface via which each can send messages or maintain a server that receives messages. Figure \ref{fig:high-level-structure} shows communication paths between each of these subsystems. \\

Each process requires additional information when initialised, such as 

\begin{enumerate}
  \item The role it should take.
  \item The address at which it should start a server to receive messages.
  \item The set of addresses of all the participating processes with which it must communicate.
\end{enumerate}

Items (1) and (2) are provided as comand line arguments when the program is executed. If the number of processes is large, then supplying a list of addresses to each process would prove tedious so (3) is addressed by using a configuration file, the relative path to which is provided as a command line argument. \\ 

Configuration files are formatted as JSON\footnote{https://tools.ietf.org/html/rfc7159}, chosen since it is semi-structured data that can be written by hand and also parsed using Yojson. The file contains, for each role, a list of all of the addresses of each process in that role. Not every process needs to know the address of every other node, it is only necessary that each file parsed by each process contains the addresses of the nodes they are required to communicate with. In practice, it is more maintainable to have a single file used by each process. \\

The addresses discussed above are IPV4 IP address and port number pairs which are used both to send and receive. Internally a different addressing format is used, described in \S\ref{section:messages}, where the addresses provided at initialisation ar mapped directly onto the internal representation. \\

\section{Data structures}

Before proceeding to the implementation of the systems described above, it is necessary to examine the data structures that will be used.

\subsection{Identifiers}

Unique identifiers are required to identify each process in the system. To avoid having a central authority distribute these identifiers a method where any process can generate their own unique identifer is used. Universally Unique Identifers\footnote{https://tools.ietf.org/html/rfc4122} (UUIDs) are used as they are a well established standard and have support in the OCaml Core library. UUIDs are also totally ordered and so satisy the condition that we have such an ordering on the identifers of leaders. \\

% Hence replicas, leaders and acceptor identifiers have the type \texttt{Core.Uuid.t}. As we wish to expose clients to a request / response protocol it is necessary to store a map of client identifiers to addresses by each replica. Rather, by modifying client identifiers to be of the form \texttt{Core.Uuid.t * Uri.t}, the address actually forms part of the identifier.

\subsection{Key value store}

\begin{figure}
  \begin{lstlisting}[mathescape=true]
module type APPLICATION = sig
  type state = (int, string) List.Assoc.t
  type operation = Nop                  
                 | Create of int * string  
                      $\smash \vdots$   
                 | Reconfigure of Uri.t list

  type result = Success 
              | Failure
              | ReadSuccess of string
                  
  val initial_state : state
  val apply : state -> operation -> state * result
         $\smash \vdots$
end
  \end{lstlisting}
    \caption{Signature of the key value store module}
    \label{fig:key-value-store}
  \centering
\end{figure}

\begin{table}
  \centering
  \begin{tabular}{l | c | p{9.0cm}}
    \textbf{Command} & \textbf{Argument} & \textbf{Semantics} \\ \hline
    Nop & & No operation, no change to state \\ 
    & & Returns Success \\ [.5\normalbaselineskip] \hline
    
    Create & \texttt{(K,V)} & If key not present, add new \texttt{(K,V)} pair to the state and return \texttt{Success}. Otherwise return \texttt{Failure}. \\ [.5\normalbaselineskip] \hline
    
    Update & \texttt{(K,V)} & If key present, update pair with key \texttt{K} to value \texttt{V}. Otherwise return \texttt{Failure} \\ [.5\normalbaselineskip] \hline
     
     Read & \texttt{K} & If key present read value \texttt{V} associated with key \texttt{K} and return \texttt{ReadSuccess(V)}, else return \texttt{Failure}. \\ [.5\normalbaselineskip] \hline
     
     Remove & \texttt{K} & If key present remove pair with key \texttt{K}, else return \texttt{Failure}. \\ [.5\normalbaselineskip] \hline
     
     Reconfigure & \texttt{uris} & Treated as a \texttt{Nop} by the application. Reconfigures the leaders to those with addresses in \texttt{uris} \\ [.5\normalbaselineskip]

  \end{tabular}
  \caption{Set of operations that can be applied to the application state, along with their corresponding arguments and their semantics. Note that Create and Update commands are separate, each with their own success and failure semantics in order to reduce ambiguity of how the application operates.}
\label{table:operation-summary}
\end{table}

The key--value store is the application to be replicated. Each replica process maintains an instance of its state. The application need not maintain any synchronisation logic, it need only behave as a state machine. The state is represented by an \emph{association list} that maps integer keys to string values. Operations each have their semantics described in Table \ref{table:operation-summary}. The \texttt{Reconfigure} command does not affect application state but is used by replicas to perform reconfigurations, described in \S\ref{section:replicas}. The application follows a state machine pattern in that if each replica starts in the same state and applies the same sequence of operations the resulting state is the same. \\

When a command has been committed to a slot by the consensus algorithm, the command's operation is applied to the state. This returns a new state and a result. The result is the value that is returned to the client; either \texttt{Success}, \texttt{Failure} or \texttt{ReadSuccess(V)}. The signature of the key value store is shown in Figure \ref{fig:key-value-store}.

\subsection{Ballots}

The definition of ballot numbers from \S\ref{subsection:multi-decree-paxos} lends itself to representation by an algebraic datatype. Ballots are hence the tagged union of Bottom (representing the least ballot $\bot$) or a pair consiting of an integer and a leader identifer. The type definition is listed in Figure \ref{fig:ballot-types}. \\

\begin{figure}
  \begin{lstlisting}
type t = Bottom
       | Number of int * leader_id
  \end{lstlisting}
    \caption{Types of ballot numbers.}
    \label{fig:ballot-types}
    
      \begin{lstlisting}
type t
val bottom : unit -> t
val init : leader_id -> t
val succ_exn : t -> t
  \end{lstlisting}
    \caption{The interface exposed by the Ballot module. These are the types of functions that can be used to generate ballot numbers. Note the naming of the function \texttt{succ\_exn} implies it can throw an exception, which occurs when calling the function on \texttt{bottom}.}
    \label{fig:ballot-interface}
  \centering
\end{figure}

Figure \ref{fig:ballot-interface} shows the interface exposed by the Ballot module. Note that the concrete type of a ballot number isn't exposed, only the abstract type \texttt{t}, hiding the internal representation of the type. This allows the type system to prevent errors arising from improper use of ballots. \\

Functions are supplied to safely manipulate ballots outside the module. A given leader can generate an initial ballot number with their identifer. Subsequent ballot numbers can be generated by calling a successor function, which increments the round number each time. By exposing such an interface we prevent errors such as using negative round numbers from being able to compile. The module provides functions to test structural equality and the ordering of ballots. Also in the Ballot module are functions for serialisation and deserialisation for use by the messaging system.

\section{Messages}
\label{section:messages}
\subsection{Interface exposed}

The messaging subsystem is concerned with the sending and receipt of messages between processes. It transforms messages from OCaml types into a form suitable for transport over the network, handling packetisation, rentramssion and masking failures. \\

\begin{figure}[t]
  \begin{lstlisting}
type non_blocking_message = ClientRequestMessage of command
                          | ProposalMessage of proposal
                          | DecisionMessage of proposal
                          | ClientResponseMessage of command_id * result
                          
val send_non_blocking : non_blocking_message -> Uri.t -> unit Lwt.t

val send_phase1_message : Ballot.t -> Uri.t -> 
  (Core.Uuid.t * Ballot.t * Pval.t list, string) Result.result Lwt.t
    
val send_phase2_message : Pval.t -> Uri.t -> 
  (Core.Uuid.t * Ballot.t, string) Result.result Lwt.t
  \end{lstlisting}
  \centering
  \caption{Types of non-blocking messages}
  \label{message-types}
\end{figure}

\begin{figure}[t]
  \begin{lstlisting}
val start_new_server : 
    ?request_callback:(command -> unit) ->
    ?proposal_callback:(proposal -> unit) ->
    ?response_callback:(command_id * result -> unit) ->
    ?phase1_callback:(Ballot.t -> 
      unique_id * Ballot.t * Pval.t list) ->
    ?phase2_callback:(Pval.t -> unique_id * Ballot.t) ->
    string -> int -> Uri.t Lwt.t  
  \end{lstlisting}
  \centering
  \caption{Function to start a new server that presents a number of possible callbacks}
  \label{fig:message-server}
\end{figure}

There are a number of different messages that are necessary to send in Multi--Paxos. In this chapter we focus on the capability of sending these messages rather than their role in the algorithm itself, which is discussed in depth later in the chapter. \\

Messages are broadly divided into two cateogries: \emph{blocking} and \emph{non-blocking}. Blocking messages are those in which the sender waits on a response, whereas when sending a non-blocking message the process does not wait for a reply. The interface for sending messages is presented in Figure \ref{message-types}. The types of non-blocking messages represent the arguments that are associated with each message. \\

The function \texttt{send\_non\_blocking} takes a value of type non-blocking message and the URI address of the destination. As these messages do not explicity require a reply they immediately return a \texttt{unit Lwt.t}. As we can regard a message that is never delivered to a recipient as indefinitely delayed in the network, this type is returned regardless of any errors actually encountered.. It is important to note here that any errors that arise from messages failing to deliver are handled by Multi--Paxos, not the messaging system. \\

Blocking messages return a type immediately. Hence they are separated into specific functions, \texttt{send\_phase1\_message} and \texttt{send\_phase2\_message}. The arguments represent the phase1a / phase2a messages and the responses represent the phase2a and phase2b messages. This reduces complexity in messaging. Since acceptors send messages only in response to messages from leaders we can treat them as executing remote procedures for leaders. The return type is wrapped in a \texttt{Result.result} to express the possibility of an error. These are discarded by leaders when checking for majority quorums. \\

The message module also exposes a function to start a new server, the type of which is listed in Figure \ref{fig:message-server}. The server requires a string and integer representing the IP address and port number on which to listen and a number of \emph{optional functions} that each represents a callback. These functions are called when a corresponding message is received by the server and return the response given by the return type. A process can therefore interoperate with the server by passing functions it wishes to have called whenever a message is received.

\subsection{Cap'n Proto}

Cap'n Proto is used as the underlying system for sending messages. Cap'n Proto requires that one writes a schema file describing a service; this contains data structures and messages that need to be serialised. This schema file, listed in full in Appendix \ref{appendix:schema}, contains an interface that describes the service made available to the messaging system. Crucially the schema contains a method for every message, the types mapping to the associated message type described in Figure \ref{message-types}. However, some of the arguments in the Cap'n Proto schema file are stored as \texttt{Text} rather than as the types in Figure \ref{message-types}, as they are serialised into JSON strings. This provides a degree of extensibility in the way messages are formatted. For example, this helps avoid situations where a new application command would require rewriting and re--compilation of the schema. \\

A compiler tool is used to generate the OCaml signatures and structures for the message API. The function \texttt{start\_new\_server} returns a service object that has a method for handling receipt of each message. In each of these methods the arguments are deserialised from the Cap'n Proto representation into the message type used by the system and the corresponding callback passed to \texttt{start\_new\_server} called. The result of calling this function is then serialised and returned as a response. \\

\begin{figure}
  \begin{lstlisting}
let service_from_uri uri =
  try Lwt.return_some (Hashtbl.find sturdy_refs uri)
  with Not_found ->
    let client_vat = Capnp_rpc_unix.client_only_vat () in
    let sr = Capnp_rpc_unix.Vat.import_exn client_vat uri in
      Sturdy_ref.connect sr >>= function
        | Ok capability ->
            Hashtbl.add sturdy_refs uri capability;
            Lwt.return_some capability
        | Error _ -> 
            Lwt.return_none
  \end{lstlisting}
  \centering
  \caption{Function that returns a Cap'n Proto capability for the message service of a given URI}
  \label{fig:service-from-uri}
\end{figure}

{\color{red}Sending a message requires translating a Cap'n Proto URI\footnote{https://tools.ietf.org/html/rfc3986} that addresses a server into a \emph{capability} which represents a stateful connection to a server. For each server initialized with the \texttt{start\_new\_server} function the supplied IP address and port number are mapped to a URI that is used to address the internal capability. Given a URI one can derive a sturdy reference to the capability and connect via that sturdy reference. Each connection runs over TCP and so rather than have each message setup and teardown a TCP session a cache of connected capabilities is stored by the messaging module that maps URIs to capabilities, listed in Figure \ref{fig:service-from-uri}. The function searches the table hashed by URIs first to see if a connection has already been established and returns that if it does. Otherwise if there is a cache miss an attempt to connect to the capability is initiated. If this results in a success then the capability cached and then returned. Note here the return type is \texttt{Capability.t option Lwt.t} since the connection could result in an error.}

% Sending a message requires translating a Cap'n Proto URI\footnote{https://tools.ietf.org/html/rfc3986} server address into a \emph{capability} that represents a stateful connection to the server. The IP address and port number with which a server is initialised is mapped onto a URI. Given a URI one can derive a sturdy reference to the capability and connect via that reference. \\

%Cap'n Proto connections run over TCP; this handles packetisation and re--transmission. ... \\

%TCP provides a means of re--transmission at the packet level. Multi--Paxos handles ... \\

%Talk about the cache of capabilities. \\

\section{Clients and replicas}

\subsection{Clients}

Clients are the simplest processes in the system as they only send commands $\left(\kappa, \textrm{cid}, \textrm{op} \right)$ to replicas in \texttt{request($\kappa, \textrm{cid}, \textrm{op}$)} messages, where $\kappa$ is an identifier of the client of type \texttt{Uuid.t * Uri.t}. The identifier value is used to uniquely identify the client and the URI is a Cap'n Proto address required for replicas to direct their responses to clients in question. \\

Each client also maintains a server on a given host name and port number to which \texttt{response($\textrm{cid}, \textrm{result}$)} messages are sent. This represents the result of command with identifier $\textrm{cid}$ having been applied to the application state, its order with all other commands having been serialised consistently across the system by Multi--Paxos.

\subsection{Replicas}
\label{section:replicas}

\begin{figure}
\begin{lstlisting}
type t = {
  id : replica_id;
  mutable app_state : app_state;
  mutable slot_in : slot_number;
  mutable slot_out : slot_number;
  mutable requests : command list;
  mutable proposals : proposal list;
  mutable decisions : proposal list;
  mutable leaders : Uri.t list;
}
\end{lstlisting}
\centering
\caption{Types of records representing replica state}
\label{fig:replica-types}
\end{figure}

Following on from clients was the implementation of replicas. The state of a replica is stored as a record with mutable fields, listed in Figure \ref{fig:replica-types}. Replicas implicitly take the implicit role of learners in order to learn the decided serialisation of client requests. Rather than have the replicas explicity request the result of the synod protocol, replicas forward the requests made by clients onto the leaders and later receive a response, learning the result implicitly. \\

Replicas maintain three queues:
\begin{itemize}
  \item \texttt{requests:} A queue of commands received from client requests.
  \item \texttt{proposals:} A queue of commands tagged with a provisional slot number, representing the replica's proposed serialisation of the commands.
  \item \texttt{decisions:} A queue containing the serialisation of commands that has been decided on by the configuration of leaders and acceptors.
\end{itemize}

These queues are stored by the replica as types \texttt{command list} and \texttt{proposal list}, as lists in OCaml have a collection of useful helper functions. Further, the type \texttt{proposal list} is structurally equivalent to \texttt{(slot\_number, command) List.Assoc.t}, allowing for proposals to be looked up by their key or inverted and looked up by their command, functionality necessary for managing the flow of proposals through the queues. \\

In order to track the next slots in which to propose and commit, each replica maintains two counters:
\begin{itemize}
  \item \texttt{slot\_in}: Represents the lowest next available slot for which no proposal has yet been allocated; that is $\nexists  \ \left( s, c \right) \in \textrm{proposals}. \ s = \textrm{slot\_in}$.
  \item \texttt{slot\_out}: Represents the lowest next available slot for which no decision has been committed; that is $\nexists \ \left( s, c \right) \in \textrm{decisions}. \ s = \textrm{slot\_out}$.
\end{itemize}

Commands and proposals move through this system of queues as shown in Figure \ref{fig:replica-queues}. \texttt{request($c$)} messages sent by clients are entered into the requests queue. Concurrently the replica will attempt to dequeue commands in a producer--consumer relationship, proposing each to \texttt{slot\_in}, the lowest free slot, and incrementing \texttt{slot\_in}. Each of these commands is tagged with this slot number and entered into the proposals queue. Once in this queue, it can then be proposed to the synod protocol by broadcasting a \texttt{propose($s$,$c$)} to the set of leaders. \\

Leaders will at some point in the future return a \texttt{decision($s$,$c'$)} message for each slot $s$ for which a command was proposed. If $c = c'$ for a proposal $\left(s,c\right) \in \textrm{proposals}$ then it is removed from proposals and added to decisions. However if $c \neq c'$ then a different command has been decided for the slot than the one proposed by this replica (i.e. the replicas disagreed on their serialisations). In this case $\left(s,c'\right)$ is committed to decisions and $c$ is returned to requests so that it can be re--proposed for a different slot number. \\

Every time that a proposal is committed in the decisions queue, the proposal $\left( \textrm{\texttt{slot\_out}}, \left( \kappa, \textrm{cid}, \textrm{op} \right) \right)$ has its operation applied, followed by \texttt{slot\_out} being incremented. Even though the decisions will be committed in the same sequence by each replica, the order in which they are committed may differ. By sweeping through the decisions queue and executing each we ensure each replica updates the application state in the same order. The application state is updated by applying op to the replica's application state, resulting in an updated state and a result res. A \texttt{response(cid,res)} message is sent to the client with identifer $\kappa$\footnote{Recall client identifiers have a URI component so responses can be sent without replicas maintaining a mapping of identifiers to URIs.}. \\

\begin{figure}
  \scalebox{0.7}{
    \input{include/replica-queues.latex}}
  \caption{Flow of requests and proposals through a replica's queues}
  \label{fig:replica-queues}
\end{figure}
  
Reconfigurations represent a change in the set of leaders and acceptors participating in the synod protocol. Replicas are expected to be able to reconfigure, that is to change the set of leaders with which they share proposals. Reconfigurations are operations that do not alter the application state, instead updating the set of leaders to a new configuration. \\

A \emph{window} is maintained, so that only a maximum \texttt{WINDOW} number of commands are ever being decided upon by the synod protocol at any given time; reconfiguration commands do not take effect until there are no more commands in flight for the previous configuration. Hence all pending proposals will have been decided by the time the reconfiguration has taken place. This requires maintaining the invariant $ \textrm{\texttt{slot\_in}} < \textrm{\texttt{slot\_out}} + \textrm{\texttt{WINDOW}} $. Therefore a maximum \texttt{WINDOW} number of proposals are being worked on by the synod protocol at once. Figure \ref{fig:replica-window} shows an example window undergoing this process. \\

\begin{figure}
  \centering
    \scalebox{0.8}{
      \input{include/window.latex}
    }
  \caption{(a) shows a window with one available slot into which a command $c_{i+3}$ is proposed, leading to the window shown in (b). In (b) however no further slots are available for proposals since $ \textrm{\texttt{slot\_in}} < \textrm{\texttt{slot\_out}} + \textrm{\texttt{WINDOW}} $. In (c), a decision is received for $c_{i+1}$ and so the \texttt{slot\_out} is incremented and now another slot is available for proposals.}
  \label{fig:replica-window}  
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Synod protocol}

Replicas each propose their own serialisation of commands; varying processing speeds and delays in the network along with crash failures of replicas can cause these serialisations to differ. It is the function of the synod protocol to return a consistent serialisation of the proposals to the replicas. This serialisation must be decided upon in a fault--tolerant manner by the leaders and acceptors. Before we examine the role that leaders play in the next step of the algorithm it is necessary to observe the functionality of acceptors.

\subsection{Acceptors}

\begin{figure}
  \begin{lstlisting}
type t = {
  id : unique_id;
  mutable ballot_num : Ballot.t;
  mutable accepted : Pval.t list
}
\end{lstlisting}
  \centering
  \caption{Types of records storing acceptor state}
  \label{fig:acceptor-type}
\end{figure}

Acceptors form a distributed fault tolerant memory of the consensus protocol. As with other processes, an acceptor's state is stored in a record with type given listed in Figure \ref{fig:acceptor-type}. Acceptors maintain the following mutable state

\begin{itemize}
  \item \texttt{ballot\_num}: The acceptor's most recently \emph{adopted} ballot number, initially equal to $\bot$ so that an acceptor adopts the first ballot it receives.
  \item \texttt{accepted}: The list of pvalues which the acceptor has \emph{accepted}, initially empty.
\end{itemize}

Acceptors represent passive processes in the algorithm. They change their state and send messages only in direct response to messages from leaders; other than when initialising, all processing occurs upon receiving a phase 1a or phase 2a message from a leader. Given this, an acceptor is just required to maintain its state and provide two callbacks, listed in Figure \ref{fig:acceptor-phase1} and \ref{fig:acceptor-phase2}.The acceptor maintains a server to receive phase 1a messages and one to receive phase2a messages. Recall from \ref{section:messages} that these are blocking messages and so the phase 1b and 2b replies are just the return type of these callbacks. \\

\begin{figure}
  \begin{lstlisting}
let phase1_callback (a : t) (b : Ballot.t) =
  Mutex.critical_section callback_mutex ~f:(fun () ->
    if a.ballot_num < b then
      a.ballot_num <- b;
    (a.id, a.ballot_num, a.accepted))
\end{lstlisting}
  \centering
  \caption{Function called when an acceptor receives a phase1a message with associated ballot.}
  \label{fig:acceptor-phase1}

  \begin{lstlisting}
let phase2_callback (a : t) (pval : Pval.t) =
  Mutex.critical_section callback_mutex ~f:(fun () ->
    let (b,_,_) = pval in
    if b = a.ballot_num then
       (if not (List.mem a.accepted pval ~equal:Pval.equal) then
         a.accepted <- pval :: a.accepted);
    (a.id, a.ballot_num))
  \end{lstlisting}
  \centering
  \caption{Function called when an acceptor receives a phase2a message with associated pvalue.}
  \label{fig:acceptor-phase2}
\end{figure}

The callbacks may be executed concurrently by the underyling server. Hence it is necessary to enclose both the callbacks within critical sections to ensure data mutated in one callback isn't used in another. The function \texttt{Mutex.critical\_section} takes a function that contains the contents of the callbacks and the mutex to lock when entering the critical section. When one function holds the lock any other callbacks will block upon attempting to hold the lock and receive the lock when the function holding the lock exits. \\

Leaders broadcast \texttt{phase1\_callback($b$)} to the acceptors when they seek adoption of ballot number $b$ from a majority quorum. In this case, when an acceptor receives a \texttt{phase1\_callback($b$)} message, it will adopt the ballot if it is greater than the last ballot it adopted (and hence inductively it is greater than any ballots adopted previously). The acceptor will then return a triple consisting of its identifier, the ballot it has adopted and the list of accepted pvalues. The identifier of the acceptor is returned so leaders can track which acceptors have replied. Rather than just not reply when it does not adopt $b$, it responds with the already adopted ballot so that leaders can be \emph{preempted} and try a higher ballot number. The list of accepted pvalues is returned so that if the acceptor has accepted any pvalues from other leaders in the past the leader attempting to secure adoption of $b$ will learn about them from this reply. \\

Leaders send \texttt{phase2\_callback(pval)} messages, where $\textrm{\texttt{pval}} = \left( b, s, c \right)$, when they believe $b$ has been adopted by a majority quorum of acceptors; this is the leaders request to commit the proposal $(s,c)$. When an acceptor receives such a message they may have adopted a higher ballot number or the message may have been delayed. Hence if the acceptor's $\textrm{\texttt{ballot\_num}} = b$ then the acceptor does accept the pvalue $(b,s,c)$ and adds it to the list of accepted pvalues; otherwise it doesn't. In either case, it returns its identifier and \texttt{ballot\_num}, so that in case the leader will know whether $(b,s,c)$ was accepted or a higher ballot had been adopted by the acceptor. \\

In this situation each acceptor may have a different list of accepted pvalues. When we note that leaders only decide that a proposal $(s,c)$ is committed after receiving a majority quorum of phase 2 messages then each such proposal must have a corresponding ballot number $(b,s,c)$ accepted a majority of acceptors. This is what gives us the fault tolerant memory; the system can tolerate the crashes of a minority of acceptors $f$ out of $2f+1$ total participating and still have a consistent memory of the sequence of proposals.

\subsection{Leaders}

Leaders receive from each replica a serialisation of the set of proposals. The leaders are required to between them, in a manner that tolerates the failure of all but one leader, return a single serialisation of the proposals to each of the replicas. \\

Leaders spawn and manage \emph{scouts} and \emph{commanders} in order to separate attempting to secure adoption of ballots and attempting to secure acceptance of pvalues. Each scout and commander is a sub--process with associated state and execution context. Rather than using the existing messaging subsystem which would produce excessive overhead for managing exchange of data between sub--processes they instead communicate locally. \\

\begin{figure}
  \begin{lstlisting}
type process_response = Adopted of Ballot.t * Pval.t list
                      | Preempted of Ballot.t    
  \end{lstlisting}
  \caption{Types of responses produced by scouts and commanders}
  \label{fig:sub-process-responses}
\end{figure}

Each sub--process has a lifetime in which it is spawned, messages acceptors, receives responses, performs some processing and then terminates. A terminating sub--process may return result of its computation to the leader that spawned it. In the case of scouts, this is either a notification of a preemption having occurred or its ballot having been adopted. Commanders may notify the leader on termination of a preemption having occured. The types of these responses are listed in Figure \ref{fig:sub-process-responses}. \\

We treat the sub--processes and the leader that spawned them as engaging in a producer--consumer relationship. Figure \ref{fig:sub-process-messaging-queue} lists code pertaining to the response queue used. Terminating sub--processes produce responses and enqueue them onto \texttt{message\_queue}, a queue of responses, via the \texttt{send : process\_response -> unit Lwt.t} function. The leader will periodically consume these responses from the queue, in turn possibly spawning more sub--processes. The queue is protected by a guard mutex \texttt{queue\_guard} that prevents races from occuring when the leader or any sub--processes attempt to concurrently mutate the state of the queue.

\begin{figure}
  \begin{lstlisting}
let queue_guard = Lwt_mutex.create ()
let message_queue : process_response Queue.t = Queue.of_list []
let send msg = 
  Lwt_mutex.lock queue_guard >|= (fun () ->
  Queue.enqueue message_queue msg) >|= fun () ->
  Lwt_mutex.unlock queue_guard
  \end{lstlisting}
  \caption{Functions and values for manipulating the message queue of process responses}
  \label{fig:sub-process-messaging-queue}
\end{figure}

\subsubsection{Scouts}

As described above, the purpose of a scout sub--process is to take a ballot number and attempt to secure adoption of that ballot number with a majority quorum of acceptors. The scout is spawned by a commander, passed this ballot number and terminates by enqueuing a \texttt{Adopted($b$,pvals)} or \texttt{Preempted($b'$)} response for the leader to consume. \\

Scouts are described by a signature listed in Figure \ref{fig:scout-sig}. Each scout's state is represented by a mutable record of type \texttt{t'}. Calling \texttt{spawn} and passing as arguments a leader and a ballot number will initialise a new scout. Since its execution is asynchronous the function returns \texttt{()} immediately. Note that the ballot number $b$ over which it seeks adoptions is immutable; even though a leader works over ever increasing ballot numbers, $b$ is fixed for the duration of the a scout's lifetime.  \\

\begin{figure}
  \begin{lstlisting}
module type SCOUT = sig
  type t' = {
    b : Ballot.t;
    acceptor_uris : Uri.t list;
    receive_lock : Lwt_mutex.t;  
    mutable pvalues : Pval.t list;
    mutable quorum : (Uri.t, unique_id) Quorum.t;
    mutable terminated : bool
  }
  val spawn : t -> Ballot.t -> unit
end
  \end{lstlisting}
  \centering
  \caption{Signature of scouts}
  \label{fig:scout-sig}
\end{figure}

Upon being spawned, a scout broadcasts, in parallel, to the acceptors a \texttt{phase1a($b$)} message. Each of these is bound monadically with a function that returns \texttt{()} if an error occured in message delivery (in keeping with the messaging semantics) or the phase 1a return value $(\alpha, b', \textrm{\texttt{pvalues}}')$. Upon receiving each message, the scout checks if it has already obtained a majority quorum of responses (not including the response just received), in which case it discards the response. \\

If $b = b'$ then the scout adds \texttt{pvalues'} to the set \texttt{pvalues} already stored and adds $\alpha$ to the quorum of responses it has received. If the scout has now obtained a majority quorum then the scout terminates, adding a \texttt{Adopted($b$,pvals)} message to the queue for a leader to consume. If not, the scout continues to wait for further responses from the acceptors. \\

If $b \neq b'$ then an acceptor has adopted a higher ballot number from another leader and so the scout terminates with a \texttt{Preempted($b'$)} message so that the leader is notified that this ballot has been abandoned.

\subsubsection{Commanders}

Commanders are spawned and passed a pvalue $\left(b, s, c \right)$. A commander spawned with this pvalue operates under the assumption phase 1 has concluded for the ballot $b$ and so the commander will attempt to secure acceptance of a pvalue with ballot number $b$. \\

It begins by broadcasting a \texttt{phase2a($b$,$s$,$c$)} message to the acceptors and waits for a response from each. It behaves in a similar pattern to a scout: If the commander already has a majority quorum of responses it discards the response. If not, then the commander examines the $\left(\alpha, b' \right)$ return type. \\

If $b = b'$ then the commander adds $\alpha$ to the quorum. If, having added $\alpha$ the commander has secured a majority quorum then the proposal $(s,c)$ has successfully been committed. The commander broadcasts to all replicas a \texttt{decision($s$,$c$)}; otherwise the commander continues to wait for further responses. \\

If $b \neq b'$ then, just as we had with scouts, the commander will terminate with a  \texttt{Preempted($b'$)} so the leader can abandon this ballot.

\subsubsection{Leaders}

Having described the operation of scouts and commanders, it is now necessary to discuss when leaders spawn sub--processes and how they process their responses. Leaders maintain the following important state:

\begin{itemize}
  \item \texttt{ballot\_num}: initially $\left(0,\lambda)$ for leader with identifier $\lambda$. This is the ballot over which the leader currently attempts to secure adoption and acceptance of a corresponding pvalue.
  \item \texttt{active}: a boolean value that describes whether the leader is currently in \emph{active} or \emph{passive} mode.
  \item \texttt{proposals}: the list of proposals the leader has received from replicas.
\end{itemize}

A passive leader has spawned a scout and is waiting for phase 1 of the synod protocol to conclude. The leader initialises in passive mode when it attempts to perform phase 1 with the ballot $\left(0, \lambda \right)$. {\color{red}Conceptually we can treat leaders as having receiving three different kinds of messages: proposals from replicas, adoption messages from scouts and preemption messages from scouts or commanders (although of course the implementation of how these are processed differs). }\\

Leaders receive \texttt{Propose($s,c$)} messages from replicas and, if they're not already present, add them to their list of proposals. If the leader is active then it will spawn a commander to attempt to commit this proposal, since a quorum of acceptors will have adopted the leader's current ballot. If the leader is not active then it simply waits until an adoption message is received, in which case it will then spawn commanders for all stored proposals. \\

Preemption messages \texttt{Preempted($b$)} occur when a sub--process receives from an acceptor a ballot number $b$ higher than \texttt{ballot\_num}. In this case the commander moves into passive mode, increments the round number of its ballot and spawns a new scout to attempt to secure adoption with this higher ballot number. \\

An \texttt{Adopted($b$,pvals)} message indicates that phase 1 of the protocol has concluded. The leader will in this situation be in passive mode and so enter active mode, beginning phase 2 of the protocol. {\color{red}From \texttt{pvals} the leader for each slot number the command with the highest corresponding ballot and adds it to their list of \texttt{proposals}, removing from \texttt{proposals} any that have the same slot number.} For each of these proposals the leader spawns a commander sub--process for each pvalue, executing phase 2 of the protocol and seeking acceptance for each.

\section{Summary}

This chapter described the implementation details of the project; beginning with a high level overview of the architecture and then delving into the message system, each role a process can take and their ability to communicate. Chapter 4 goes on to describe the evaluation of the software that took place.








